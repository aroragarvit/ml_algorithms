{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data set\n",
      "(4238, 16)\n",
      "Nan values in train data \n",
      " male                 0\n",
      "age                  0\n",
      "education           86\n",
      "currentSmoker        0\n",
      "cigsPerDay          27\n",
      "BPMeds              41\n",
      "prevalentStroke      0\n",
      "prevalentHyp         0\n",
      "diabetes             0\n",
      "totChol             41\n",
      "sysBP                0\n",
      "diaBP                0\n",
      "BMI                 16\n",
      "heartRate            1\n",
      "glucose            315\n",
      "TenYearCHD           0\n",
      "dtype: int64\n",
      "---------------------------------------------------------\n",
      "Nan values in test data \n",
      " male                0\n",
      "age                 0\n",
      "education          19\n",
      "currentSmoker       0\n",
      "cigsPerDay          2\n",
      "BPMeds             12\n",
      "prevalentStroke     0\n",
      "prevalentHyp        0\n",
      "diabetes            0\n",
      "totChol             9\n",
      "sysBP               0\n",
      "diaBP               0\n",
      "BMI                 3\n",
      "heartRate           0\n",
      "glucose            73\n",
      "TenYearCHD          0\n",
      "dtype: int64\n",
      "Now nan values in train are\n",
      "0\n",
      "Now nan values in test are\n",
      "0\n",
      "(741, 1)\n"
     ]
    }
   ],
   "source": [
    "# Generate train as well as test csv files after cleansing of the data by framingham university for relations between colestrol and hear disease \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.mode.chained_assignment = None  # To suppress a warning \n",
    "from sklearn.model_selection import train_test_split\n",
    "data=pd.read_csv(\"framingham.csv\")\n",
    "print(\"Shape of data set\")\n",
    "print(data.shape)\n",
    "train,test = train_test_split(data,test_size=0.2,shuffle=True,random_state=1)  # splitting the data with shuffling at  shuffling or dropping indexes are not reseted accordingly   and setting random_state so that after shuffling any time data split is same \n",
    "print(\"Nan values in train data \\n {}\".format(train.isna().sum()))\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(\"Nan values in test data \\n {}\".format(test.isna().sum()))\n",
    "\n",
    "train=train.dropna(axis=0, how='any')\n",
    "print(\"Now nan values in train are\")\n",
    "print(train.isna().sum().sum())\n",
    "train.insert(0,'ones',1)    # inserting 1 at first column for our vectorised implementation \n",
    "train=train.to_csv(\"training_data.csv\",index=False)\n",
    "\n",
    "test.dropna(axis=0, how='any', inplace=True)   # removing rows (axis = 0)\n",
    "print(\"Now nan values in test are\")\n",
    "print(test.isna().sum().sum())\n",
    "test.insert(0,'ones',1)\n",
    "test=test.to_csv(\"testing_data.csv\",index=False)\n",
    "\n",
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for column in df.columns:\n",
    "        max_value = df[column].max()\n",
    "        min_value = df[column].min()\n",
    "        if(max_value-min_value!=0):\n",
    "            result[column] = (df[column] - min_value) / (max_value - min_value) \n",
    "        else:\n",
    "            result[column]=df[column]\n",
    "    return(result) \n",
    "\n",
    "\n",
    "normalised_train=normalize(pd.read_csv(\"training_data.csv\"))\n",
    "normalised_test=normalize(pd.read_csv(\"testing_data.csv\"))\n",
    "\n",
    "normalised_test_data=normalised_test.iloc[:,0:16].to_numpy()\n",
    "test_result=normalised_test.iloc[:,16].to_numpy()\n",
    "test_result=test_result[:,np.newaxis] \n",
    "print(test_result.shape)  \n",
    "\n",
    "\n",
    "\n",
    "normalised_train_data=normalised_train.iloc[:,0:16].to_numpy()\n",
    "\n",
    "train_result=normalised_train.iloc[:,16].to_numpy()\n",
    "train_result=train_result[:,np.newaxis]             ### VERY VERYVERY IMPORTANT ALWAYS CONVERT YOUT VECTOR TO A  MATRIX FOR SURE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN 0 epoch loss is [1.89368292] and accuracy is 15.643224699828473\n",
      "IN 1 epoch loss is [1.77058747] and accuracy is 15.643224699828473\n",
      "IN 2 epoch loss is [1.65317848] and accuracy is 15.643224699828473\n",
      "IN 3 epoch loss is [1.54191723] and accuracy is 15.643224699828473\n",
      "IN 4 epoch loss is [1.43720648] and accuracy is 15.643224699828473\n",
      "IN 5 epoch loss is [1.3393669] and accuracy is 15.643224699828473\n",
      "IN 6 epoch loss is [1.24861659] and accuracy is 15.643224699828473\n",
      "IN 7 epoch loss is [1.16505624] and accuracy is 15.643224699828473\n",
      "IN 8 epoch loss is [1.08866229] and accuracy is 15.643224699828473\n",
      "IN 9 epoch loss is [1.01928919] and accuracy is 15.643224699828473\n",
      "IN 10 epoch loss is [0.95668039] and accuracy is 15.643224699828473\n",
      "IN 11 epoch loss is [0.90048661] and accuracy is 15.917667238421956\n",
      "IN 12 epoch loss is [0.85028848] and accuracy is 16.946826758147512\n",
      "IN 13 epoch loss is [0.80562092] and accuracy is 20.171526586620928\n",
      "IN 14 epoch loss is [0.7659963] and accuracy is 27.51286449399657\n",
      "IN 15 epoch loss is [0.73092469] and accuracy is 37.97598627787307\n",
      "IN 16 epoch loss is [0.69992986] and accuracy is 50.01715265866209\n",
      "IN 17 epoch loss is [0.6725609] and accuracy is 64.1852487135506\n",
      "IN 18 epoch loss is [0.64839974] and accuracy is 75.67753001715266\n",
      "IN 19 epoch loss is [0.62706508] and accuracy is 80.20583190394511\n",
      "IN 20 epoch loss is [0.60821374] and accuracy is 82.3327615780446\n",
      "IN 21 epoch loss is [0.59153993] and accuracy is 83.25900514579759\n",
      "IN 22 epoch loss is [0.57677333] and accuracy is 83.67066895368782\n",
      "IN 23 epoch loss is [0.5636764] and accuracy is 83.87650085763293\n",
      "IN 24 epoch loss is [0.55204127] and accuracy is 84.11663807890223\n",
      "IN 25 epoch loss is [0.54168663] and accuracy is 84.18524871355059\n",
      "IN 26 epoch loss is [0.53245459] and accuracy is 84.21955403087479\n",
      "IN 27 epoch loss is [0.52420785] and accuracy is 84.39108061749572\n",
      "IN 28 epoch loss is [0.51682707] and accuracy is 84.49399656946827\n",
      "IN 29 epoch loss is [0.51020848] and accuracy is 84.45969125214408\n",
      "IN 30 epoch loss is [0.50426183] and accuracy is 84.35677530017152\n",
      "IN 31 epoch loss is [0.49890854] and accuracy is 84.4253859348199\n",
      "IN 32 epoch loss is [0.49408014] and accuracy is 84.49399656946827\n",
      "IN 33 epoch loss is [0.48971684] and accuracy is 84.52830188679246\n",
      "IN 34 epoch loss is [0.48576642] and accuracy is 84.52830188679246\n",
      "IN 35 epoch loss is [0.48218313] and accuracy is 84.52830188679246\n",
      "IN 36 epoch loss is [0.47892688] and accuracy is 84.49399656946827\n",
      "IN 37 epoch loss is [0.47596242] and accuracy is 84.4253859348199\n",
      "IN 38 epoch loss is [0.47325877] and accuracy is 84.39108061749572\n",
      "IN 39 epoch loss is [0.4707886] and accuracy is 84.39108061749572\n",
      "IN 40 epoch loss is [0.46852777] and accuracy is 84.39108061749572\n",
      "IN 41 epoch loss is [0.46645494] and accuracy is 84.35677530017152\n",
      "IN 42 epoch loss is [0.46455119] and accuracy is 84.35677530017152\n",
      "IN 43 epoch loss is [0.46279973] and accuracy is 84.35677530017152\n",
      "IN 44 epoch loss is [0.46118562] and accuracy is 84.35677530017152\n",
      "IN 45 epoch loss is [0.45969556] and accuracy is 84.39108061749572\n",
      "IN 46 epoch loss is [0.45831771] and accuracy is 84.45969125214408\n",
      "IN 47 epoch loss is [0.45704145] and accuracy is 84.45969125214408\n",
      "IN 48 epoch loss is [0.45585732] and accuracy is 84.45969125214408\n",
      "IN 49 epoch loss is [0.4547568] and accuracy is 84.45969125214408\n",
      "IN 50 epoch loss is [0.45373227] and accuracy is 84.45969125214408\n",
      "IN 51 epoch loss is [0.4527769] and accuracy is 84.4253859348199\n",
      "IN 52 epoch loss is [0.4518845] and accuracy is 84.4253859348199\n",
      "IN 53 epoch loss is [0.45104953] and accuracy is 84.4253859348199\n",
      "IN 54 epoch loss is [0.45026697] and accuracy is 84.4253859348199\n",
      "IN 55 epoch loss is [0.4495323] and accuracy is 84.4253859348199\n",
      "IN 56 epoch loss is [0.44884144] and accuracy is 84.4253859348199\n",
      "IN 57 epoch loss is [0.44819067] and accuracy is 84.39108061749572\n",
      "IN 58 epoch loss is [0.44757665] and accuracy is 84.39108061749572\n",
      "IN 59 epoch loss is [0.44699632] and accuracy is 84.39108061749572\n",
      "IN 60 epoch loss is [0.44644692] and accuracy is 84.39108061749572\n",
      "IN 61 epoch loss is [0.44592594] and accuracy is 84.39108061749572\n",
      "IN 62 epoch loss is [0.44543109] and accuracy is 84.39108061749572\n",
      "IN 63 epoch loss is [0.4449603] and accuracy is 84.39108061749572\n",
      "IN 64 epoch loss is [0.44451166] and accuracy is 84.39108061749572\n",
      "IN 65 epoch loss is [0.44408344] and accuracy is 84.39108061749572\n",
      "IN 66 epoch loss is [0.44367406] and accuracy is 84.39108061749572\n",
      "IN 67 epoch loss is [0.44328208] and accuracy is 84.39108061749572\n",
      "IN 68 epoch loss is [0.44290618] and accuracy is 84.4253859348199\n",
      "IN 69 epoch loss is [0.44254515] and accuracy is 84.4253859348199\n",
      "IN 70 epoch loss is [0.44219788] and accuracy is 84.39108061749572\n",
      "IN 71 epoch loss is [0.44186337] and accuracy is 84.39108061749572\n",
      "IN 72 epoch loss is [0.44154067] and accuracy is 84.39108061749572\n",
      "IN 73 epoch loss is [0.44122894] and accuracy is 84.39108061749572\n",
      "IN 74 epoch loss is [0.44092739] and accuracy is 84.39108061749572\n",
      "IN 75 epoch loss is [0.44063531] and accuracy is 84.39108061749572\n",
      "IN 76 epoch loss is [0.44035204] and accuracy is 84.39108061749572\n",
      "IN 77 epoch loss is [0.44007695] and accuracy is 84.39108061749572\n",
      "IN 78 epoch loss is [0.43980951] and accuracy is 84.39108061749572\n",
      "IN 79 epoch loss is [0.43954918] and accuracy is 84.39108061749572\n",
      "IN 80 epoch loss is [0.4392955] and accuracy is 84.39108061749572\n",
      "IN 81 epoch loss is [0.43904803] and accuracy is 84.39108061749572\n",
      "IN 82 epoch loss is [0.43880637] and accuracy is 84.39108061749572\n",
      "IN 83 epoch loss is [0.43857014] and accuracy is 84.39108061749572\n",
      "IN 84 epoch loss is [0.43833899] and accuracy is 84.39108061749572\n",
      "IN 85 epoch loss is [0.43811262] and accuracy is 84.39108061749572\n",
      "IN 86 epoch loss is [0.43789073] and accuracy is 84.39108061749572\n",
      "IN 87 epoch loss is [0.43767304] and accuracy is 84.39108061749572\n",
      "IN 88 epoch loss is [0.43745932] and accuracy is 84.39108061749572\n",
      "IN 89 epoch loss is [0.43724932] and accuracy is 84.39108061749572\n",
      "IN 90 epoch loss is [0.43704283] and accuracy is 84.39108061749572\n",
      "IN 91 epoch loss is [0.43683965] and accuracy is 84.39108061749572\n",
      "IN 92 epoch loss is [0.4366396] and accuracy is 84.39108061749572\n",
      "IN 93 epoch loss is [0.43644252] and accuracy is 84.39108061749572\n",
      "IN 94 epoch loss is [0.43624823] and accuracy is 84.39108061749572\n",
      "IN 95 epoch loss is [0.43605659] and accuracy is 84.39108061749572\n",
      "IN 96 epoch loss is [0.43586748] and accuracy is 84.39108061749572\n",
      "IN 97 epoch loss is [0.43568076] and accuracy is 84.39108061749572\n",
      "IN 98 epoch loss is [0.43549631] and accuracy is 84.39108061749572\n",
      "IN 99 epoch loss is [0.43531403] and accuracy is 84.39108061749572\n",
      "IN 100 epoch loss is [0.43513382] and accuracy is 84.39108061749572\n",
      "IN 101 epoch loss is [0.43495558] and accuracy is 84.39108061749572\n",
      "IN 102 epoch loss is [0.43477922] and accuracy is 84.39108061749572\n",
      "IN 103 epoch loss is [0.43460467] and accuracy is 84.39108061749572\n",
      "IN 104 epoch loss is [0.43443184] and accuracy is 84.39108061749572\n",
      "IN 105 epoch loss is [0.43426068] and accuracy is 84.39108061749572\n",
      "IN 106 epoch loss is [0.4340911] and accuracy is 84.39108061749572\n",
      "IN 107 epoch loss is [0.43392306] and accuracy is 84.39108061749572\n",
      "IN 108 epoch loss is [0.43375648] and accuracy is 84.39108061749572\n",
      "IN 109 epoch loss is [0.43359133] and accuracy is 84.39108061749572\n",
      "IN 110 epoch loss is [0.43342756] and accuracy is 84.39108061749572\n",
      "IN 111 epoch loss is [0.4332651] and accuracy is 84.39108061749572\n",
      "IN 112 epoch loss is [0.43310393] and accuracy is 84.39108061749572\n",
      "IN 113 epoch loss is [0.432944] and accuracy is 84.39108061749572\n",
      "IN 114 epoch loss is [0.43278527] and accuracy is 84.39108061749572\n",
      "IN 115 epoch loss is [0.43262771] and accuracy is 84.39108061749572\n",
      "IN 116 epoch loss is [0.43247129] and accuracy is 84.39108061749572\n",
      "IN 117 epoch loss is [0.43231597] and accuracy is 84.39108061749572\n",
      "IN 118 epoch loss is [0.43216172] and accuracy is 84.39108061749572\n",
      "IN 119 epoch loss is [0.43200853] and accuracy is 84.39108061749572\n",
      "IN 120 epoch loss is [0.43185635] and accuracy is 84.39108061749572\n",
      "IN 121 epoch loss is [0.43170517] and accuracy is 84.39108061749572\n",
      "IN 122 epoch loss is [0.43155497] and accuracy is 84.39108061749572\n",
      "IN 123 epoch loss is [0.43140572] and accuracy is 84.39108061749572\n",
      "IN 124 epoch loss is [0.43125741] and accuracy is 84.39108061749572\n",
      "IN 125 epoch loss is [0.43111001] and accuracy is 84.39108061749572\n",
      "IN 126 epoch loss is [0.43096351] and accuracy is 84.39108061749572\n",
      "IN 127 epoch loss is [0.43081789] and accuracy is 84.39108061749572\n",
      "IN 128 epoch loss is [0.43067313] and accuracy is 84.39108061749572\n",
      "IN 129 epoch loss is [0.43052923] and accuracy is 84.39108061749572\n",
      "IN 130 epoch loss is [0.43038616] and accuracy is 84.39108061749572\n",
      "IN 131 epoch loss is [0.43024391] and accuracy is 84.39108061749572\n",
      "IN 132 epoch loss is [0.43010247] and accuracy is 84.39108061749572\n",
      "IN 133 epoch loss is [0.42996183] and accuracy is 84.39108061749572\n",
      "IN 134 epoch loss is [0.42982197] and accuracy is 84.39108061749572\n",
      "IN 135 epoch loss is [0.42968289] and accuracy is 84.39108061749572\n",
      "IN 136 epoch loss is [0.42954458] and accuracy is 84.39108061749572\n",
      "IN 137 epoch loss is [0.42940702] and accuracy is 84.39108061749572\n",
      "IN 138 epoch loss is [0.4292702] and accuracy is 84.39108061749572\n",
      "IN 139 epoch loss is [0.42913412] and accuracy is 84.39108061749572\n",
      "IN 140 epoch loss is [0.42899877] and accuracy is 84.39108061749572\n",
      "IN 141 epoch loss is [0.42886414] and accuracy is 84.39108061749572\n",
      "IN 142 epoch loss is [0.42873022] and accuracy is 84.39108061749572\n",
      "IN 143 epoch loss is [0.428597] and accuracy is 84.39108061749572\n",
      "IN 144 epoch loss is [0.42846448] and accuracy is 84.39108061749572\n",
      "IN 145 epoch loss is [0.42833265] and accuracy is 84.39108061749572\n",
      "IN 146 epoch loss is [0.4282015] and accuracy is 84.39108061749572\n",
      "IN 147 epoch loss is [0.42807103] and accuracy is 84.39108061749572\n",
      "IN 148 epoch loss is [0.42794123] and accuracy is 84.39108061749572\n",
      "IN 149 epoch loss is [0.42781209] and accuracy is 84.39108061749572\n",
      "IN 150 epoch loss is [0.42768361] and accuracy is 84.39108061749572\n",
      "IN 151 epoch loss is [0.42755578] and accuracy is 84.39108061749572\n",
      "IN 152 epoch loss is [0.4274286] and accuracy is 84.39108061749572\n",
      "IN 153 epoch loss is [0.42730206] and accuracy is 84.39108061749572\n",
      "IN 154 epoch loss is [0.42717616] and accuracy is 84.39108061749572\n",
      "IN 155 epoch loss is [0.42705089] and accuracy is 84.39108061749572\n",
      "IN 156 epoch loss is [0.42692624] and accuracy is 84.39108061749572\n",
      "IN 157 epoch loss is [0.42680222] and accuracy is 84.39108061749572\n",
      "IN 158 epoch loss is [0.42667881] and accuracy is 84.39108061749572\n",
      "IN 159 epoch loss is [0.42655602] and accuracy is 84.39108061749572\n",
      "IN 160 epoch loss is [0.42643384] and accuracy is 84.39108061749572\n",
      "IN 161 epoch loss is [0.42631225] and accuracy is 84.39108061749572\n",
      "IN 162 epoch loss is [0.42619127] and accuracy is 84.39108061749572\n",
      "IN 163 epoch loss is [0.42607088] and accuracy is 84.39108061749572\n",
      "IN 164 epoch loss is [0.42595109] and accuracy is 84.39108061749572\n",
      "IN 165 epoch loss is [0.42583188] and accuracy is 84.39108061749572\n",
      "IN 166 epoch loss is [0.42571325] and accuracy is 84.39108061749572\n",
      "IN 167 epoch loss is [0.4255952] and accuracy is 84.39108061749572\n",
      "IN 168 epoch loss is [0.42547773] and accuracy is 84.39108061749572\n",
      "IN 169 epoch loss is [0.42536083] and accuracy is 84.39108061749572\n",
      "IN 170 epoch loss is [0.4252445] and accuracy is 84.39108061749572\n",
      "IN 171 epoch loss is [0.42512874] and accuracy is 84.39108061749572\n",
      "IN 172 epoch loss is [0.42501353] and accuracy is 84.39108061749572\n",
      "IN 173 epoch loss is [0.42489888] and accuracy is 84.39108061749572\n",
      "IN 174 epoch loss is [0.42478479] and accuracy is 84.39108061749572\n",
      "IN 175 epoch loss is [0.42467124] and accuracy is 84.39108061749572\n",
      "IN 176 epoch loss is [0.42455825] and accuracy is 84.39108061749572\n",
      "IN 177 epoch loss is [0.42444579] and accuracy is 84.39108061749572\n",
      "IN 178 epoch loss is [0.42433388] and accuracy is 84.39108061749572\n",
      "IN 179 epoch loss is [0.42422251] and accuracy is 84.39108061749572\n",
      "IN 180 epoch loss is [0.42411166] and accuracy is 84.39108061749572\n",
      "IN 181 epoch loss is [0.42400135] and accuracy is 84.39108061749572\n",
      "IN 182 epoch loss is [0.42389157] and accuracy is 84.39108061749572\n",
      "IN 183 epoch loss is [0.42378231] and accuracy is 84.39108061749572\n",
      "IN 184 epoch loss is [0.42367358] and accuracy is 84.39108061749572\n",
      "IN 185 epoch loss is [0.42356536] and accuracy is 84.39108061749572\n",
      "IN 186 epoch loss is [0.42345765] and accuracy is 84.39108061749572\n",
      "IN 187 epoch loss is [0.42335046] and accuracy is 84.39108061749572\n",
      "IN 188 epoch loss is [0.42324378] and accuracy is 84.39108061749572\n",
      "IN 189 epoch loss is [0.42313761] and accuracy is 84.39108061749572\n",
      "IN 190 epoch loss is [0.42303193] and accuracy is 84.39108061749572\n",
      "IN 191 epoch loss is [0.42292676] and accuracy is 84.39108061749572\n",
      "IN 192 epoch loss is [0.42282209] and accuracy is 84.39108061749572\n",
      "IN 193 epoch loss is [0.42271791] and accuracy is 84.39108061749572\n",
      "IN 194 epoch loss is [0.42261422] and accuracy is 84.39108061749572\n",
      "IN 195 epoch loss is [0.42251103] and accuracy is 84.39108061749572\n",
      "IN 196 epoch loss is [0.42240831] and accuracy is 84.39108061749572\n",
      "IN 197 epoch loss is [0.42230609] and accuracy is 84.39108061749572\n",
      "IN 198 epoch loss is [0.42220434] and accuracy is 84.39108061749572\n",
      "IN 199 epoch loss is [0.42210307] and accuracy is 84.39108061749572\n",
      "------------------------After training weights are as following --------------------------------\n",
      "[[-1.60907892]\n",
      " [ 0.0888136 ]\n",
      " [-0.03158265]\n",
      " [-0.28533518]\n",
      " [-0.14850724]\n",
      " [-0.12972214]\n",
      " [ 0.44232402]\n",
      " [ 0.23152235]\n",
      " [ 0.06625814]\n",
      " [ 0.57432666]\n",
      " [ 0.08076223]\n",
      " [ 0.49973769]\n",
      " [-0.2529402 ]\n",
      " [-0.39897005]\n",
      " [ 0.3110927 ]\n",
      " [ 0.70817352]]\n",
      "-------------------------------------------------------------------------------------------------\n",
      "testing accuracy is  ______________________________\n",
      "86.5047233468286\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Losses=[]\n",
    "AccuracyPercentage=[]\n",
    "Epoches=[]\n",
    "class logistic_regression:\n",
    "      def __init__(self,train,result,test,test_result,epoches,alpha):\n",
    "         self.train=train    #//  m*n+1\n",
    "         self.result=result  #//  m*1\n",
    "         self.test=test\n",
    "         self.test_result=test_result\n",
    "         samples,features=train.shape             # features will also contain 1-1-1-1 in them as first  column \n",
    "         self.samples=samples\n",
    "         self.features=features\n",
    "         self.weights=np.random.rand(features,1)  # n+1*1 # it will contain a single column and number of rows same as number of features\n",
    "         self.epoches=epoches\n",
    "         self.alpha=alpha\n",
    "\n",
    "      def sigmoid(self,x):\n",
    "           y=1/(1+np.exp(-x) )                       \n",
    "           return y\n",
    "      \n",
    "      \n",
    "      def predict(self, X):        \n",
    "        \n",
    "        z = np.dot(X, self.weights) #m*1\n",
    "        \n",
    "        array= np.array([1 if i > 0.5 else 0 for i in self.sigmoid(z)]) # Returning binary result\n",
    "        array=array[:,np.newaxis]\n",
    "        return array\n",
    "\n",
    "      def accuracy(self ,y, y_hat):\n",
    "        accuracy_per = (np.sum(y == y_hat) / len(y) ) * 100\n",
    "        return accuracy_per\n",
    "\n",
    "\n",
    "      def training(self):\n",
    "        \n",
    "          for i in range(self.epoches):  # in each epoch weights are updated and new predictions are made on which new error is generated which is then optimised again again with another loop \n",
    "              Epoches.append(i)\n",
    "              predicted_data=np.dot(self.train,self.weights)  \n",
    "              predicted_probability = self.sigmoid(predicted_data)  #  m*1\n",
    "              binary_predicted=self.predict(self.train)              # self.weights are cahnging in each loop so our binary prediction will also be cahnging in each loop\n",
    "              accuracy_percentage=self.accuracy(self.result,binary_predicted)\n",
    "              AccuracyPercentage.append(accuracy_percentage)\n",
    "             \n",
    "                                                            # -1/m *summation(i=1 to m) { y[i] log(h_theta(x[i]) + (1-y[i]) log(1-h_theta[x[i]]))}\n",
    "                                                            # in logistic regression even if one prediction is not correct then our error will go to nan or infinity \n",
    "                                                            # calculating error using matrices only and not applying the loop \n",
    "              loss= -1/self.samples * np.sum  ( np.add (np.multiply(self.result, np.log(predicted_probability)), np.multiply(1-self.result, np.log(1-predicted_probability)) ) , axis=0)\n",
    "              Losses.append(loss)\n",
    "              print(\"IN {} epoch loss is {} and accuracy is {}\".format(i,loss,accuracy_percentage))\n",
    "              weights_subtract= self.alpha/self.samples * np.dot(np.transpose(self.train),np.subtract( predicted_probability,self.result) )\n",
    "              \n",
    "              self.weights=np.subtract(self.weights,weights_subtract)\n",
    "# also testing  could be done in each loop\n",
    "      def testing(self):\n",
    "        binary_pre=self.predict(self.test)\n",
    "        acc_per=self.accuracy(self.test_result,binary_pre)\n",
    "        print(\"testing accuracy is  ______________________________\")\n",
    "        print(acc_per)\n",
    "\n",
    "        \n",
    "              \n",
    "a= logistic_regression(train=normalised_train_data,result=train_result,test=normalised_test_data,test_result=test_result,epoches=200,alpha=0.1)\n",
    "\n",
    "a.training()\n",
    "print(\"------------------------After training weights are as following --------------------------------\")\n",
    "print(a.weights)\n",
    "print(\"-------------------------------------------------------------------------------------------------\")\n",
    "a.testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.title(\"Loss With Each Epoch\",loc = 'left')\n",
    "plt.xlabel(\"epoches\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.scatter(Epoches,Losses)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.title(\"Accuracy With Each Epoch\",loc = 'left')\n",
    "plt.xlabel(\"epoches\")\n",
    "plt.ylabel(\"Accuracy% ge\")\n",
    "plt.scatter(Epoches,AccuracyPercentage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ae54068e622f61d47b992eee920515c98b314ebd4be04c0239c7d903a657733"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
